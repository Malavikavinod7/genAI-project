{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "!pip install wget | tail -n 1\n!pip install scikit-learn | tail -n 1\n!pip install \"ibm-watson-machine-learning>=1.0.310\" | tail -n 1", "metadata": {"id": "0edfe851-6047-4e19-be32-ca603f7aa5b9"}, "outputs": [{"name": "stdout", "text": "Successfully installed wget-3.2\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages (from lomond->ibm-watson-machine-learning>=1.0.310) (1.16.0)\n", "output_type": "stream"}], "execution_count": 1}, {"cell_type": "code", "source": "url = \"https://us-south.ml.cloud.ibm.com\"\napikey='AcSmvalVuyr5r5PkpmIuQGX7fxkfDPw_MmSvusXFd27a '\n", "metadata": {"id": "a315a448-709c-46f4-a783-aae0f62bbd64"}, "outputs": [], "execution_count": 2}, {"cell_type": "code", "source": "credentials = {\n    \"url\": url,\n    \"apikey\": apikey\n}\n", "metadata": {"id": "ca351cf3-3e55-42b0-8a83-55b1bf903e95"}, "outputs": [], "execution_count": 3}, {"cell_type": "code", "source": "import os\n\ntry:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")\n\n\nproject_id\n", "metadata": {"id": "606c104b-4f34-466a-9841-93681e09430d"}, "outputs": [{"execution_count": 4, "output_type": "execute_result", "data": {"text/plain": "'3b65b630-96b0-413b-bbd3-5511f839b826'"}, "metadata": {}}], "execution_count": 4}, {"cell_type": "code", "source": "import types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\nfrom io import StringIO # Import StringIO\n\ncos_client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='AcSmvalVuyr5r5PkpmIuQGX7fxkfDPw_MmSvusXFd27a',  # Your secret API key\n    ibm_auth_endpoint='https://iam.cloud.ibm.com/oidc/token',\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.us-south.cloud-object-storage.appdomain.cloud'\n)\n\nbucket = 'bucket-n38x7k26jbac15c'\nobject_key = 'legal_sentiment_dataset (1).csv'\n\n# Get the object content\nbody = cos_client.get_object(Bucket=bucket, Key=object_key)['Body']\n\n# Read the content into a string\ncsv_string = body.read().decode('utf-8')\n\n# Use StringIO to treat the string as a file\ndata = pd.read_csv(StringIO(csv_string))\n\ndata.head(10)", "metadata": {"id": "4aebe93b-6a43-4ecb-a9c5-5779e500b1f3"}, "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "   ID                                            Phrase  Sentiment\n0   1             The plaintiff's claims are dismissed.         -1\n1   2     The contract is deemed valid and enforceable.          1\n2   3        The appeal is denied due to lack of merit.         -1\n3   4     The legal team submitted additional evidence.          0\n4   5                The appeal is under consideration.          0\n5   6  The settlement agreement is fair and reasonable.          1\n6   7                The appeal is under consideration.          0\n7   8  The settlement agreement is fair and reasonable.          1\n8   9             The court reviewed the documentation.          0\n9  10     The contract is deemed valid and enforceable.          1", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Phrase</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>The plaintiff's claims are dismissed.</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>The contract is deemed valid and enforceable.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>The appeal is denied due to lack of merit.</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>The legal team submitted additional evidence.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>The appeal is under consideration.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>The settlement agreement is fair and reasonable.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>The appeal is under consideration.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>The settlement agreement is fair and reasonable.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>The court reviewed the documentation.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>The contract is deemed valid and enforceable.</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "execution_count": 8}, {"cell_type": "code", "source": "label_map={\n    -1:'negative',\n     0:'netural',\n     1: 'positive'\n}", "metadata": {"id": "bf9229c5-c729-4701-aabb-ff7214124eee"}, "outputs": [], "execution_count": 37}, {"cell_type": "code", "source": "data.value_counts(['Sentiment'])", "metadata": {"id": "b59ee2b3-dd4f-4b33-a121-a97e35cf0dc3"}, "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "Sentiment\n 1           184\n-1           167\n 0           149\nName: count, dtype: int64"}, "metadata": {}}], "execution_count": 9}, {"cell_type": "code", "source": "from sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndata_train, data_test, y_train, y_test = train_test_split(\n    data['Phrase'],\n    data['Sentiment'],\n    test_size=0.3,\n    random_state=33,\n    stratify=data['Sentiment']\n)\n\n# Convert to DataFrames\ndata_train = pd.DataFrame({'Phrase': data_train, 'Sentiment': y_train})\ndata_test = pd.DataFrame({'Phrase': data_test, 'Sentiment': y_test})", "metadata": {"id": "7389a82d-3fd0-4404-8048-414dfb272d12"}, "outputs": [], "execution_count": 10}, {"cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n\nprint([model.name for model in ModelTypes])", "metadata": {"id": "2a5799b5-7d03-4421-bd6a-8be6ff777a70"}, "outputs": [{"name": "stdout", "text": "['FLAN_T5_XXL', 'FLAN_UL2', 'MT0_XXL', 'GPT_NEOX', 'MPT_7B_INSTRUCT2', 'STARCODER', 'LLAMA_2_70B_CHAT', 'LLAMA_2_13B_CHAT', 'GRANITE_13B_INSTRUCT', 'GRANITE_13B_CHAT', 'FLAN_T5_XL', 'GRANITE_13B_CHAT_V2', 'GRANITE_13B_INSTRUCT_V2', 'ELYZA_JAPANESE_LLAMA_2_7B_INSTRUCT', 'MIXTRAL_8X7B_INSTRUCT_V01_Q', 'CODELLAMA_34B_INSTRUCT_HF', 'GRANITE_20B_MULTILINGUAL']\n", "output_type": "stream"}], "execution_count": 11}, {"cell_type": "code", "source": "model_id = ModelTypes.FLAN_T5_XXL", "metadata": {"id": "53f3faa8-2353-4f09-bc95-11e9e496ef17"}, "outputs": [], "execution_count": 12}, {"cell_type": "code", "source": "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n\nparameters = {\n    GenParams.DECODING_METHOD: \"greedy\",\n    GenParams.RANDOM_SEED: 33,\n    GenParams.REPETITION_PENALTY: 1,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 1\n}", "metadata": {"id": "008638ee-e94a-468e-9ad3-2dfcafbbda29"}, "outputs": [], "execution_count": 13}, {"cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models import Model\n\n# Ensure these are correctly defined\nmodel_id = \"google/flan-ul2\" # Example model ID\nparameters = {\n    \"decoding_method\": \"greedy\",\n    \"min_new_tokens\": 10,\n    \"max_new_tokens\": 50\n}\ncredentials = {\n    \"apikey\": \"AcSmvalVuyr5r5PkpmIuQGX7fxkfDPw_MmSvusXFd27a\", # Replace with your actual API key\n    \"url\": \"https://us-south.ml.cloud.ibm.com\" # Replace with your WML service endpoint URL\n}\nproject_id = \"3b65b630-96b0-413b-bbd3-5511f839b826\" # Replace with your actual project ID\n\ntry:\n    model = Model(\n        model_id=model_id,\n        params=parameters,\n        credentials=credentials,\n        project_id=project_id\n    )\n    print(\"Model object initialized successfully!\")\n    # You can now use the 'model' object for inference\nexcept Exception as e:\n    print(f\"An error occurred during model initialization: {e}\")", "metadata": {"id": "97d233ad-73a1-47f6-9397-ee55a21d5bf8"}, "outputs": [{"name": "stdout", "text": "Model object initialized successfully!\n", "output_type": "stream"}, {"name": "stderr", "text": "/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_watson_machine_learning/foundation_models/utils/utils.py:273: LifecycleWarning: Model 'google/flan-ul2' is in deprecated state from 2025-05-28 until 2025-07-30. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n  warnings.warn(default_warning_template.format(\n", "output_type": "stream"}], "execution_count": 16}, {"cell_type": "code", "source": "model.get_details()", "metadata": {"id": "f51b64eb-b515-4c7f-bcd1-12e19220d10f"}, "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "{'model_id': 'google/flan-ul2',\n 'label': 'flan-ul2-20b',\n 'provider': 'Google',\n 'source': 'Hugging Face',\n 'functions': [{'id': 'autoai_rag'}, {'id': 'text_generation'}],\n 'short_description': 'flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.',\n 'long_description': 'flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.',\n 'terms_url': 'https://huggingface.co/google/flan-ul2/blob/main/README.md',\n 'input_tier': 'class_3',\n 'output_tier': 'class_3',\n 'number_params': '20b',\n 'min_shot_size': 0,\n 'task_ids': ['question_answering',\n  'summarization',\n  'retrieval_augmented_generation',\n  'classification',\n  'generation',\n  'extraction',\n  'translation'],\n 'tasks': [{'id': 'question_answering', 'ratings': {'quality': 4}},\n  {'id': 'summarization', 'ratings': {'quality': 4}},\n  {'id': 'retrieval_augmented_generation', 'ratings': {'quality': 4}},\n  {'id': 'classification', 'ratings': {'quality': 4}},\n  {'id': 'generation'},\n  {'id': 'extraction', 'ratings': {'quality': 4}},\n  {'id': 'translation'}],\n 'model_limits': {'max_sequence_length': 4096, 'max_output_tokens': 4095},\n 'limits': {'lite': {'call_time': '5m0s', 'max_output_tokens': 4095},\n  'v2-professional': {'call_time': '10m0s', 'max_output_tokens': 4095},\n  'v2-standard': {'call_time': '10m0s', 'max_output_tokens': 4095}},\n 'lifecycle': [{'id': 'available', 'start_date': '2023-07-07'},\n  {'id': 'deprecated', 'start_date': '2025-05-28'},\n  {'id': 'withdrawn', 'start_date': '2025-07-30'}]}"}, "metadata": {}}], "execution_count": 17}, {"cell_type": "code", "source": "instruction = \"\"\"Determine the sentiment of the sentence.\nUse either 'positive', 'negative', 'neutral'. Use the provided examples as a template.\"\"\"", "metadata": {"id": "dd4d04cf-5d33-4b9e-a112-9883251be865"}, "outputs": [], "execution_count": 18}, {"cell_type": "code", "source": "zero_shot_inputs = [{\"input\": text} for text in data_test['Phrase']]\nfor i in range(2):\n    print(f\"The sentence example {i+1} is:\\n\\t {zero_shot_inputs[i]['input']}\\n\")", "metadata": {"id": "f43d6814-44fe-451a-b11d-4b4b4cec16ad"}, "outputs": [{"name": "stdout", "text": "The sentence example 1 is:\n\t The legal team submitted additional evidence.\n\nThe sentence example 2 is:\n\t The appeal is denied due to lack of merit.\n\n", "output_type": "stream"}], "execution_count": 19}, {"cell_type": "code", "source": "data_train_and_labels = data_train.copy()\ndata_train_and_labels['Sentiment'] = y_train", "metadata": {"id": "f470eec3-ce04-4e5f-a2de-bad0dcd6ba39"}, "outputs": [], "execution_count": 20}, {"cell_type": "code", "source": "few_shot_example = []\nfew_shot_examples = []", "metadata": {"id": "87f7a190-1acc-4702-b411-5326c86e20a5"}, "outputs": [], "execution_count": 21}, {"cell_type": "code", "source": "for phrase, sentiment in data_train_and_labels \\\n    .groupby('Sentiment') \\\n    .apply(lambda x: x.sample(2)).values:\n    \n    few_shot_example.append(f\"\\tsentence:\\t{phrase}\\n\\tsentiment: {sentiment}\\n\")\n\nfew_shot_examples = '\\n'.join(few_shot_example)", "metadata": {"id": "07b7989d-8d71-4649-baa4-c8f56f214c95"}, "outputs": [], "execution_count": 22}, {"cell_type": "code", "source": "few_shot_inputs_ = [{\"input\": text} for text in data_test['Phrase'].values]", "metadata": {"id": "e7d3ffc8-6674-494d-b875-48c2c500fe02"}, "outputs": [], "execution_count": 23}, {"cell_type": "code", "source": "for i in range(2):\n    print(f\"The sentence example {i+1} is:\\n {few_shot_inputs_[i]['input']}\\n\")\n    print(f\"\\tSentiment: {y_test.iloc[i]}\\n\")\n", "metadata": {"id": "204d2244-6589-44e1-9c49-943471443c4b"}, "outputs": [{"name": "stdout", "text": "The sentence example 1 is:\n The legal team submitted additional evidence.\n\n\tSentiment: 0\n\nThe sentence example 2 is:\n The appeal is denied due to lack of merit.\n\n\tSentiment: -1\n\n", "output_type": "stream"}], "execution_count": 25}, {"cell_type": "code", "source": "results = []\nfor inp in few_shot_inputs_[:2]:\n    results.append(\n        model.generate(\" \".join([instruction + few_shot_examples[0], inp['input']]))[\"results\"][0]\n    )\n", "metadata": {"id": "bdce05eb-2f0e-4036-b7d4-eade23151684"}, "outputs": [], "execution_count": 26}, {"cell_type": "code", "source": "import json\nprint(json.dumps(results, indent=2))", "metadata": {"id": "ab8edcf9-e5c2-465e-a8b7-0901131727d0"}, "outputs": [{"name": "stdout", "text": "[\n  {\n    \"generated_text\": \"neutral.. . . . . . . . . . . . . . . . . . . . . . . . \",\n    \"generated_token_count\": 50,\n    \"input_token_count\": 42,\n    \"stop_reason\": \"max_tokens\"\n  },\n  {\n    \"generated_text\": \"negative.. . . . . . . . . . . . . . . . . . . . . . . . \",\n    \"generated_token_count\": 50,\n    \"input_token_count\": 45,\n    \"stop_reason\": \"max_tokens\"\n  }\n]\n", "output_type": "stream"}], "execution_count": 27}, {"cell_type": "code", "source": "y_pred = [result['generated_text'] for result in results]", "metadata": {"id": "e87ce834-36bf-468f-9494-4c7d2a8c911c"}, "outputs": [], "execution_count": 35}, {"cell_type": "code", "source": "y_true = [label_map[label] for label in y_test.values[:2]]", "metadata": {"id": "06d409cc-d368-4269-b932-9f4440e6a535"}, "outputs": [], "execution_count": 38}, {"cell_type": "code", "source": "few_shot_examples_str = '\\n'.join(few_shot_example) \n\nresults = []\nfor inp_dict in few_shot_inputs_: # Iterate over ALL test inputs\n    # Construct the full prompt for each test input\n    full_prompt = f\"{instruction}\\n\\n{few_shot_examples_str}\\n\\nText: {inp_dict['input']}\\nSentiment:\"\n\n    try:\n        # Generate prediction\n        generated_text = model.generate_text(prompt=full_prompt)\n        results.append({'generated_text': generated_text.strip()}) \n    except Exception as e:\n        print(f\"Error generating for input: {inp_dict['input']} - {e}\")\n        results.append({'generated_text': 'ERROR'}) \n\ny_pred = [result['generated_text'] for result in results]", "metadata": {"id": "8431d48f-7dd6-4b10-bb9d-cc760873d140"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "cleaned_y_pred = []\nfor pred in y_pred:\n    pred_lower = pred.lower().strip()\n    if 'positive' in pred_lower:\n        cleaned_y_pred.append('positive')\n    elif 'negative' in pred_lower:\n        cleaned_y_pred.append('negative')\n    elif 'neutral' in pred_lower or 'netural' in pred_lower: # Handle typo if model generates it\n        cleaned_y_pred.append('neutral')\n    else:\n        cleaned_y_pred.append('unknown') # Handle cases where model doesn't give expected output", "metadata": {"id": "3f9ecf6b-d2a0-455b-8f01-320617c4abcd"}, "outputs": [], "execution_count": 48}, {"cell_type": "code", "source": "y_true_strings = [label_map[label] for label in y_test.values] # Apply to full y_test", "metadata": {"id": "106e3b1a-fc42-43b2-9211-990c74f4808f"}, "outputs": [], "execution_count": 41}, {"cell_type": "code", "source": "data_train, data_test, y_train, y_test = train_test_split(\n    data['Phrase'],\n    data['Sentiment'],\n    test_size=0.3,\n    random_state=33,\n    stratify=data['Sentiment']\n)\n\n# Convert to DataFrames\ndata_train = pd.DataFrame({'Phrase': data_train, 'Sentiment': y_train})\ndata_test = pd.DataFrame({'Phrase': data_test, 'Sentiment': y_test})\n\n\ny_true_strings = [label_map[label] for label in data_test['Sentiment'].values] # <--- FIX IS HERE\n\n# --- Verify lengths again (after the fix) ---\nprint(f\"\\nLength of data_test['Phrase']: {len(data_test['Phrase'])}\")\nprint(f\"Length of data_test['Sentiment']: {len(data_test['Sentiment'])}\") # Should now be 150\nprint(f\"Length of y_true_strings: {len(y_true_strings)}\") # Should now be 150\nprint(f\"Length of cleaned_y_pred: {len(cleaned_y_pred)}\") # Should be 150 if inference worked fully\n\n# --- Create DataFrame and Save ---\n# Now, all lengths should match, and this will execute without error\nresults_df = pd.DataFrame({\n    'Phrase': data_test['Phrase'].tolist(),\n    'True_Sentiment': y_true_strings,\n    'Predicted_Sentiment': cleaned_y_pred\n})\n\ncsv_file_name = 'legal_sentiment_analysis_results.csv'\nresults_df.to_csv(csv_file_name, index=False)\nprint(f\"\\nFull results saved to {csv_file_name}\")\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(\"\\n--- Evaluation Results ---\")\nprint(\"Accuracy:\", accuracy_score(y_true_strings, cleaned_y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_true_strings, cleaned_y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_true_strings, cleaned_y_pred))", "metadata": {"id": "c9661e8a-be89-4d6f-83f7-0c7f1847d975"}, "outputs": [{"name": "stdout", "text": "\nLength of data_test['Phrase']: 150\nLength of data_test['Sentiment']: 150\nLength of y_true_strings: 150\nLength of cleaned_y_pred: 150\n\nFull results saved to legal_sentiment_analysis_results.csv\n\n--- Evaluation Results ---\nAccuracy: 0.38666666666666666\n\nClassification Report:\n               precision    recall  f1-score   support\n\n    negative       1.00      0.58      0.73        50\n     netural       0.00      0.00      0.00        45\n     neutral       0.00      0.00      0.00         0\n    positive       1.00      0.53      0.69        55\n     unknown       0.00      0.00      0.00         0\n\n    accuracy                           0.39       150\n   macro avg       0.40      0.22      0.28       150\nweighted avg       0.70      0.39      0.50       150\n\n\nConfusion Matrix:\n [[29  0 12  0  9]\n [ 0  0 45  0  0]\n [ 0  0  0  0  0]\n [ 0  0 26 29  0]\n [ 0  0  0  0  0]]\n", "output_type": "stream"}, {"name": "stderr", "text": "/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n", "output_type": "stream"}], "execution_count": 47}, {"cell_type": "code", "source": "import pandas as pd\n# Create a DataFrame from your results\nresults_df = pd.DataFrame({\n    'Phrase': data_test['Phrase'].tolist(), # Use the full test phrases\n    'True_Sentiment': [lable_map[label] for label in y_test.values], # Use the full y_test\n    'Predicted_Sentiment': cleaned_y_pred # This should contain predictions for the full test set\n})\n\n# Save to CSV\nresults_df.to_csv('sentiment_analysis_results.csv', index=False)\nprint(\"Results saved to sentiment_analysis_results.csv\")", "metadata": {"id": "02753885-560f-4aff-a91c-774a2dcd9d8d"}, "outputs": [{"name": "stdout", "text": "Results saved to sentiment_analysis_results.csv\n", "output_type": "stream"}], "execution_count": 46}, {"cell_type": "code", "source": "", "metadata": {"id": "afb2e499-98c4-45d1-b364-8b573d066a5a"}, "outputs": [], "execution_count": null}]}